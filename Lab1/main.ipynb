{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71982eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mahha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c4962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nettoyage(corpus_ensemble_documents):\n",
    "\n",
    "    for i in range(len(corpus_ensemble_documents)):\n",
    "        corpus_ensemble_documents[i] = corpus_ensemble_documents[i].lower()\n",
    "    \n",
    "    \n",
    "    for i in range(len(corpus_ensemble_documents)):\n",
    "        for c in string.punctuation:\n",
    "            x = corpus_ensemble_documents[i].replace(c, ' ')\n",
    "            corpus_ensemble_documents[i] = x\n",
    "            \n",
    "    \n",
    "    stopwords_anglais = stopwords.words('english')\n",
    "    for i in range(len(corpus_ensemble_documents)):\n",
    "        L = corpus_ensemble_documents[i].split()\n",
    "        \n",
    "        L = [mot for mot in L if mot not in stopwords_anglais]\n",
    "        corpus_ensemble_documents[i] = \" \".join(L)\n",
    "    return corpus_ensemble_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b17fb22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample document', 'another document punctuation']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nettoyage([\"This is a sample document.\", \"Another document, with punctuation!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ebfe69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18dab66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF(terme, corpus, numero_document):\n",
    "    x = corpus[numero_document].split().count(terme)\n",
    "    y = len(corpus[numero_document].split())\n",
    "    return x / y if y > 0 else 0\n",
    "\n",
    "def IDF(terme, corpus, numero_document):\n",
    "    \"\"\"\n",
    "    D : nombre total de documents dans le corpus\n",
    "    d_t : nombre de documents dans lesquels le terme t appara√Æt\n",
    "    This function calculates the Inverse Document Frequency (IDF) for a given term in a corpus of documents. \n",
    "    The IDF is a measure of how important a term is within the corpus.\n",
    "    \"\"\"\n",
    "    D = len(corpus)\n",
    "    d_t = 0\n",
    "    for document in corpus:\n",
    "        if terme in document:\n",
    "            d_t += 1\n",
    "    TF_val = TF(terme, corpus, numero_document)\n",
    "    return TF_val * log(1+(D/d_t))\n",
    "\n",
    "def cles_correspondante_a_valeur(valeur, dictionnaire):\n",
    "    for cle in dictionnaire.keys():\n",
    "        if dictionnaire[cle] == valeur:\n",
    "            return cle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ed3a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrice_sparce(dictionnaire, corpus_ensemble_documents):\n",
    "    M = zeros((len(corpus_ensemble_documents), len(dictionnaire.values())))\n",
    "    for i in range(len(corpus_ensemble_documents)):\n",
    "        for j in dictionnaire.values():\n",
    "            x = cles_correspondante_a_valeur(j, dictionnaire)\n",
    "            M[i, j] = IDF(x, corpus_ensemble_documents, i)\n",
    "    return M\n",
    "\n",
    "def affiche(M):\n",
    "    (n, p) = M.shape\n",
    "    for i in range(n):\n",
    "        for j in range(p):\n",
    "            M[i, j] = round(M[i, j], 2)\n",
    "    print(M)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c986d618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionnaire des mots :  {'la': 4, 'vie': 8, 'est': 3, 'douce': 2, 'tranquille': 7, 'belle': 0, 'le': 5, 'corona': 1, 'virus': 9, 'mechant': 6}\n",
      "liste des mots :  ['la', 'vie', 'est', 'douce', 'tranquille', 'belle', 'le', 'corona', 'virus', 'mechant']\n",
      "Matrice_sparce_method predifine: \n",
      "\n",
      "[[0.   0.   0.53 0.41 0.53 0.   0.   0.   0.53 0.  ]\n",
      " [0.38 0.   0.29 0.68 0.29 0.   0.   0.38 0.29 0.  ]\n",
      " [0.   0.48 0.   0.28 0.   0.48 0.48 0.   0.   0.48]]\n",
      "Matrice_sparce obtenue par notre methode: \n",
      "\n",
      "[[0.   0.   0.23 0.17 0.23 0.   0.   0.   0.23 0.  ]\n",
      " [0.17 0.   0.11 0.26 0.11 0.   0.   0.17 0.11 0.  ]\n",
      " [0.   0.28 0.   0.14 0.   0.18 0.28 0.   0.   0.28]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "text = [\"La vie est douce\", \"La vie est tranquille, est belle, est douce\", \"le corona-virus est mechant\"]\n",
    "\n",
    "texte = nettoyage(text)\n",
    "vect = TfidfVectorizer() \n",
    "T = vect.fit_transform(texte)\n",
    "dictionnaire_des_mots = vect.vocabulary_\n",
    "print(\"dictionnaire des mots : \", dictionnaire_des_mots)\n",
    "liste_des_mots = list(dictionnaire_des_mots.keys())\n",
    "print(\"liste des mots : \", liste_des_mots)\n",
    "Matrice_sparce_correspondante = T.toarray()\n",
    "print(\"Matrice_sparce_method predifine: \\n\")\n",
    "affiche(Matrice_sparce_correspondante)\n",
    "print(\"Matrice_sparce obtenue par notre methode: \\n\")\n",
    "affiche(matrice_sparce(dictionnaire_des_mots, texte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "849a493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7bd4acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 0.9853883619584722\n",
      "Score test: 0.9784688995215312\n"
     ]
    }
   ],
   "source": [
    "spams = pd.read_table(\"SMSSmapCollection.txt\", sep='\\t', header=None)\n",
    "spams.columns = ['classe', 'message']\n",
    "\n",
    "spams = spams.dropna(subset=['message'])\n",
    "\n",
    "spamsTrain, spamsTest = train_test_split(spams, train_size=0.7, random_state=1)\n",
    "\n",
    "parseur = TfidfVectorizer(binary=False)\n",
    "XTrain = parseur.fit_transform(spamsTrain['message'])\n",
    "XTest = parseur.transform(spamsTest['message'])\n",
    "\n",
    "modelFirst = LogisticRegression()\n",
    "modelFirst.fit(XTrain, spamsTrain['classe'])\n",
    "\n",
    "score1 = modelFirst.score(XTrain, spamsTrain['classe'])\n",
    "score2 = modelFirst.score(XTest, spamsTest['classe'])\n",
    "\n",
    "print(\"Score train:\", score1)\n",
    "print(\"Score test:\", score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf1d81ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score test avec CountVectorizer: 0.9844497607655502\n",
      "Score train avec CountVectorizer: 0.9987182773647783\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "parseurBis = CountVectorizer(stop_words='english')\n",
    "XTrainBis = parseurBis.fit_transform(spamsTrain['message'])\n",
    "mdtTrainBis = XTrainBis.toarray()\n",
    "modelBis = LogisticRegression()\n",
    "modelBis.fit(mdtTrainBis, spamsTrain['classe'])\n",
    "mdtTestBis = parseurBis.transform(spamsTest['message'])\n",
    "score3 = modelBis.score(mdtTestBis, spamsTest['classe'])\n",
    "print(\"Score test avec CountVectorizer:\", score3)\n",
    "score4 = modelBis.score(mdtTrainBis, spamsTrain['classe'])\n",
    "print(\"Score train avec CountVectorizer:\", score4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9925120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score test avec TfidfVectorizer: 0.9784688995215312\n",
      "Score train avec TfidfVectorizer: 0.9853883619584722\n"
     ]
    }
   ],
   "source": [
    "parseurTfidf = TfidfVectorizer()\n",
    "XTrain3 = parseurTfidf.fit_transform(spamsTrain['message'])\n",
    "mdtTrain3 = XTrain3.toarray()\n",
    "model3 = LogisticRegression()\n",
    "model3.fit(mdtTrain3, spamsTrain['classe'])\n",
    "mdtTest3 = parseurTfidf.transform(spamsTest['message'])\n",
    "score5 = model3.score(mdtTest3, spamsTest['classe'])\n",
    "print(\"Score test avec TfidfVectorizer:\", score5)\n",
    "score6 = model3.score(mdtTrain3, spamsTrain['classe'])\n",
    "print(\"Score train avec TfidfVectorizer:\", score6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
